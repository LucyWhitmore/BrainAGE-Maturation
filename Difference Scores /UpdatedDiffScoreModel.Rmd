---
title: "Train BrainAGE with tidymodels"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
```


```{r}
library(tidymodels)
library(xgboost)
```

```{r}
load("/Volumes/devbrainlab/Lucy/BrainAGE/FYP/Difference Scores/sample_train_diff_v2.Rda")

#training_subset_test <- training_sample_baseline[1:100,]


# Model prep: split, preprocessing, CV ------------------------------------

# Train / test split ------------------------------------------------------

set.seed(42)
df_split <- initial_split(
  sample_train_diff_v2, 
  prop = 0.80,
  # matching age distributions across train and test set
  strata = "interview_age.y"
)
df_train <- training(df_split)
df_validation <- testing(df_split)

```


# Pre-processing setup ----------------------------------------------------

```{r}
# define (what we want to do)
preprocess_recipe <- df_train %>%
  # predict scan age by all brain features
  recipe(interview_age.y ~ .) %>%
  # remove near zero variance predictors
  step_nzv(all_predictors()) %>%
  prep() # where it all gets calculated

preprocess_recipe


# Apply pre-processing ----------------------------------------------------

# juice() will work with training data, `bake()` to apply this to our test data

# apply on train (gives processed value)
df_train_prep <- juice(preprocess_recipe)

# apply on validation
df_validation_prep <- preprocess_recipe %>% bake(df_validation)

```


```{r}
boost_mod <- boost_tree(
  mode = "regression", 
  trees = 150, 
  tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
  # randomness
  sample_size = tune(), mtry = tune(), 
  # step size
  learn_rate = tune()
) %>%
  set_engine("xgboost", 
             objective = "reg:squarederror")

```


```{r}
set.seed(42)

xgboost_grid <- grid_latin_hypercube(
  min_n(), 
  tree_depth(), 
  loss_reduction(),
  sample_size = sample_prop(),
  # has unknown, finalize with data to find max
  finalize(mtry(), df_train_prep),
  learn_rate(),
  size = 500 
)

```

```{r}
xgb_wf <- workflow() %>%
  add_formula(interview_age.y ~ .) %>%
  add_model(boost_mod)

xgb_wf
```


```{r}
set.seed(42)

train_cv <- df_train_prep %>%
  vfold_cv(
    v = 10, 
    repeats = 10, 
    strata = interview_age.y
  )
```

```{r}
doParallel::registerDoParallel()

set.seed(42)

xgb_tuned_results <- tune_grid(
  xgb_wf,
  resamples = train_cv,
  grid = xgboost_grid,
  metrics = metric_set(mae, rmse, rsq),
  control = control_grid(verbose = TRUE,
                         save_pred = TRUE)
)

xgb_tuned_results
```

```{r}
xgb_tuned_results %>%
  # want to minimize MAE
  show_best("mae") 

# select parsimonious params within one SE of best model
best_xgb_params <- xgb_tuned_results %>%
  select_by_one_std_err(metric = "mae", maximize = FALSE, tree_depth) 

```

Finalize workflow and fit final model
```{r}
final_xgb_diff <- finalize_workflow(
  xgb_wf,
  best_xgb_params
)

final_xgb_diff

fit_workflow_diff <- fit(final_xgb_diff, df_train_prep)


#xgb_final_mod_test <- boost_mod_test %>%
 # finalize_model(best_xgb_params_test) %>%
 # fit(interview_age ~ .,
#      data = df_train_prep)

```

Variable importance, ignore
```{r}
library(vip)

final_xgb_diff %>%
  fit(data = df_train) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point")

```

```{r}
final_res <- last_fit(final_xgb_diff, df_split)

collect_metrics(final_res)
```

Save models
```{r}
# three different save formats just in case
# 1. Rds
save(fit_workflow_diff, file = "fit_workflow_diff.rds")
# 2. rda
save(fit_workflow_diff, file = "fit_workflow_2.rda")
# 3. raw xbg object, which can then be loaded with xgb.load
model_obj_diff <- fit_workflow_diff$fit$fit$fit
xgb.save(model_obj_diff, "model_obj_diff")
```


```{r}

model_obj <- fit_workflow$fit$fit$fit
xgb.save(model_obj, "model_obj")


xgb.save.raw(model_obj, "model_obj_raw")
```

Predict
redo this part as needed when the model's done
```{r}
load("/Volumes/devbrainlab/Lucy/BrainAGE/FYP/Difference Scores/sample_test_diff_v2.Rda")

# dou
analysis_sample_diff <- sample_test_diff_v2 %>% 
  select(-c(src_subject_id, interview_age.y_diff))


#workflow_predict <- predict(fit_workflow, analysis_sample)

brain_age_diff <- fit_workflow_diff %>%
  predict(new_data = analysis_sample_diff) %>%
  mutate(
    # provide the chronological age at time of scan
    truth = analysis_sample_diff$interview_age
  ) %>%
  # compute the brain age gap by subtracting chronological age from prediction
  mutate(gap = .pred - truth)
```

Alternate Prediction, if using raw model object 
```{r}
brain_age_df_test <-
  model_obj %>%
  predict(newdata = as.matrix(analysis_sample %>% select(-c(interview_age)))) 


brain_age_df_test <- as.data.frame(brain_age_df_test) %>%
  mutate(
    # provide the chronological age at time of scan
    truth = analysis_sample_baseline$interview_age
  ) %>%
  # compute the brain age gap by subtracting chronological age from prediction
  mutate(gap = .pred - truth)

```

Bias Correction
```{r}
#run model on validation set
brain_age_diff_correction <-
  fit_workflow_diff %>%
  predict(new_data = df_validation) %>%
  mutate(
    # provide the chronological age at time of scan
    truth = df_validation$interview_age
  ) %>%
  # compute the brain age gap by subtracting chronological age from prediction
  mutate(gap = .pred - truth)


#get MAE for hold-out 
brain_age_diff_correction %>%
  metrics(truth = truth, estimate = .pred)

diff_bias_mod <- lm(.pred ~ truth, data = brain_age_diff_correction)

# extract intercept and slope
#diff_bias_mod$coefficients[["(Intercept)"]]
diff_bias_intercept <- 10.27455
#diff_bias_mod$coefficients[["truth"]]
diff_bias_slope <- 0.1411708

  
# create bias correct data frame
#Baseline
brain_age_diff_corrected_df <- brain_age_diff %>%
  mutate(
    # corrected brain age prediction
    corrected_pred =  (.pred - diff_bias_intercept) / diff_bias_slope
  ) %>%
  # corrected corrected brain age gap
  mutate(corrected_gap = corrected_pred - truth)

```

Save corrected estimates
```{r}
save(brain_age_diff_corrected_df, file="Corrected_Diff_BrainAGE.Rda")
```